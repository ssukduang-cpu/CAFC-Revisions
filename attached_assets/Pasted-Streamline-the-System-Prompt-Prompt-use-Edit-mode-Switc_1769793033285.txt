Streamline the System Prompt Prompt (use Edit mode): “Switch to Edit mode. In backend/chat.py, refactor the SYSTEM_PROMPT (lines 401-800) to a condensed version that preserves all core elements: agentic reasoning loop, core functions, grounding rules (merge rules 1, 4, and 5 into one ‘Textual Grounding Mandate’), citation formats, and confidence tiers. Move non-core details like detailed confidence tiers to a separate cached_definitions string referenced in enhanced_prompt. Replace the current definition with this: SYSTEM_PROMPT = ‘’‘You are a specialized legal research assistant for U.S. Federal Circuit patent litigators. Your only authoritative knowledge source is the opinion excerpts provided in the current conversation. Operate with litigation-grade rigor and strict textual grounding. 0. AGENTIC REASONING & REFLECTION LOOP (MANDATORY - EXECUTE BEFORE EVERY RESPONSE) [Keep the existing loop instructions here] I. CORE FUNCTION Extract holdings, standards, and operative rules; explain practitioner usage without summarizing cases. II. STRICT GROUNDING RULES (MANDATORY) Textual Grounding Mandate: ONLY use information in provided excerpts. Every statement MUST be supported by at least one VERBATIM QUOTE. If unsupported, respond ONLY with: NOT FOUND IN PROVIDED OPINIONS. Do NOT use external knowledge, assumptions, or inferences. [Keep citation format and confidence tiers here]’’’ Ensure the new prompt is ~50% shorter. Test token count using count_tokens before and after.”
2.  Enhance Retrieval with Hybrid Semantic Search Prompt (use Full or Build mode, as this may involve multi-file changes): “In backend/db_postgres.py, modify search_pages to add hybrid semantic search. Assume pgvector is enabled; generate embeddings during ingestion if needed. Update the function to compute weighted ranking: 0.6 * ts_rank + 0.4 * cosine similarity using sentence-transformers. Add an optional hybrid flag defaulting to False, increase limit to 30, then filter to top 20. Replace the existing search logic with this structure: def search_pages(query: str, opinion_ids: Optional[List[str]] = None, limit: int = 20, …): # Existing to_tsquery logic fulltext_results = … if hybrid: query_embedding = get_embedding(query)  # Implement or import vector_results = vector_search(query_embedding) combined = hybrid_rank(fulltext_results, vector_results, weights=(0.6, 0.4)) return combined[:limit] Ensure compatibility with existing calls.”
3.  Incorporate Full Multi-Turn History with Summarization Fallback Prompt (use Edit mode): “In backend/chat.py (around lines 2943-2960), update the messages list in the OpenAI API call to append the last 3-5 full messages from history. Add a check: if total tokens > 20,000 (using count_tokens), summarize older messages. Replace the messages construction with: messages = [ {‘role’: ‘system’, ‘content’: enhanced_prompt}, ] recent_history = get_recent_messages(conversation_id, max_turns=5) for msg in recent_history: messages.append({‘role’: msg[‘role’], ‘content’: msg[‘content’]}) if count_tokens(str(messages)) > 20000: summary = summarize_old_history(…) messages.insert(1, {‘role’: ‘system’, ‘content’: f’Summary of prior context: {summary}’}) messages.append({‘role’: ‘user’, ‘content’: message})”
4.  Increase Response Token Limit Dynamically Prompt (use Fast or Edit mode): “In backend/chat.py (lines 2943-2960), make max_tokens dynamic: base=1500 + len(opinion_ids) * 500, capped at 4000. Update the API call: max_tokens = min(4000, 1500 + len(opinion_ids or []) * 500) response = await asyncio.wait_for( … lambda: client.chat.completions.create( …, max_tokens=max_tokens, … ) ) Add logging for truncation risks.”
5.  Augment Quote Extraction with LLM Assistance Prompt (use Edit mode): “In backend/chat.py (lines 1110-1206), enhance extract_quotable_passages with optional LLM mode using GPT-4o-mini. Add parameter llm_mode: bool = False. If True, prompt GPT-4o-mini to extract up to 7 passages. Fallback to heuristics. Increase max_len=400. Replace the function core with: if llm_mode: passages = llm_extract_passages(page_text)  # New helper else: # Existing heuristic scoring Then rank and return top 7”
6.  Add Token Overflow Safeguards Prompt (use Edit mode): “In backend/chat.py (lines 1208-1264), add pre-build check in build_context_with_quotes: if count_tokens(context) > 80000: pages = sorted(pages, key=lambda p: p[‘score’], reverse=True) while count_tokens(context) > 80000 and pages: pages.pop()  # Remove lowest context = rebuild_context(pages) log_pruning() Return warning if pruned.”
7.  Update Parameters for GPT-4o Prompt (use Fast mode): “In backend/chat.py (lines 2943-2960), change temperature to 0.1 and increase timeouts: temperature=0.1, timeout=90.0  # For API call And in asyncio.wait_for: timeout=120.0 Make model configurable via env var, default ‘gpt-4o’.”