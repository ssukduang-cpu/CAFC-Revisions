 #7 - Lower Temperature + Longer Timeouts (Priority 1: Trivial effort, quick win for reduced hallucinations) Prompt (use Fast mode): “Switch to Fast mode. In backend/chat.py (lines 2943-2960), update the GPT-4o parameters to lower temperature to 0.1 for more deterministic outputs and increase timeouts for reliability. Replace the relevant lines with: response = await asyncio.wait_for( loop.run_in_executor( _executor, lambda: client.chat.completions.create( model=‘gpt-4o’, messages=[…],  # Existing messages temperature=0.1, max_tokens=2500, timeout=90.0 ) ), timeout=120.0 ) Make the model configurable via an environment variable, defaulting to ‘gpt-4o’. Test with a sample query to confirm no errors.”
2.  #4 - Dynamic max_tokens (Priority 2: Easy effort, better handling of complex queries) Prompt (use Edit mode): “Switch to Edit mode. In backend/chat.py (lines 2943-2960), implement dynamic max_tokens scaling based on the number of opinion_ids, with a base of 1500 and addition of 500 per ID, capped at 4000. Add this calculation before the API call and update: max_tokens = min(4000, 1500 + len(opinion_ids or []) * 500) response = await asyncio.wait_for( … lambda: client.chat.completions.create( …, max_tokens=max_tokens, … ) ) Include logging to warn if max_tokens approaches 4000. Ensure compatibility with existing parameters.”
3.  #6 - Score-Based Pruning for Token Overflow Safeguards (Priority 3: Easy to moderate effort, smarter context management) Prompt (use Edit mode): “Switch to Edit mode. In backend/chat.py (lines 1208-1264), enhance the existing token overflow check (around lines 1255-1257) to sort pages by relevance score before pruning. Replace the check with: current_tokens = count_tokens(context) if current_tokens > 80000: pages = sorted(pages, key=lambda p: p.get(‘score’, 0), reverse=True)  # Assuming ‘score’ from search_pages while current_tokens > 80000 and len(pages) > 0: removed = pages.pop() context = rebuild_context(pages)  # Assume or implement rebuild function current_tokens = count_tokens(context) logging.warning(f’Pruned page from {removed.get(“opinion_id”)}: Hit 80,000 token limit’) If pruning occurs, append a warning to the response content: ‘Context truncated due to token limits; lower-ranked pages omitted.’ Test with a high-volume query.”
4.  #1 - Streamline the System Prompt (Priority 4: 1-2 hours effort, reduced token overhead) Prompt (use Edit mode): “Switch to Edit mode. In backend/chat.py (lines 401-800), condense SYSTEM_PROMPT to approximately 50% length while retaining the agentic reasoning loop, core functions, merged grounding rules (combine 1, 4, 5 into ‘Textual Grounding Mandate’), citation formats, and confidence tiers. Move detailed confidence tier explanations to a new cached_definitions string, referenced in enhanced_prompt as ‘{cached_definitions}’. Replace with this structure: cached_definitions = ‘’’[Detailed confidence tiers here, e.g., STRONG: Binding precedent…]’’’ SYSTEM_PROMPT = ‘’‘You are a specialized legal research assistant for U.S. Federal Circuit patent litigators. Your only source is provided opinion excerpts. Operate with litigation-grade rigor. 0. AGENTIC REASONING & REFLECTION LOOP (MANDATORY) [… existing loop instructions …] I. CORE FUNCTION Extract holdings, standards, and rules; explain practitioner use without summaries. II. STRICT GROUNDING RULES Textual Grounding Mandate: ONLY use excerpt information. Support every statement with VERBATIM QUOTE. If unsupported: NOT FOUND IN PROVIDED OPINIONS. No external knowledge or inferences. [… condensed citation format …] Confidence Tiers: {cached_definitions}’’’ Update enhanced_prompt to include cached_definitions. Compare token counts using count_tokens.”
5.  #5 - LLM-Assisted Quote Extraction (Fallback Only) (Priority 5: Moderate effort, quality improvement with controlled cost) Prompt (use Edit mode): “Switch to Edit mode. In backend/chat.py (lines 1110-1206), modify extract_quotable_passages to use GPT-4o-mini as a fallback only when heuristic extraction yields fewer than 3 passages. Add logic: heuristic_passages = existing_heuristic_extraction(page_text)  # Existing code if len(heuristic_passages) < 3: llm_passages = llm_extract_passages(page_text, model=‘gpt-4o-mini’, max_passages=7, max_len=400)  # New helper function passages = heuristic_passages + llm_passages[:7 - len(heuristic_passages)] else: passages = heuristic_passages[:5] Implement llm_extract_passages as a simple OpenAI call with prompt: ‘Extract up to 7 quotable legal passages from this text, ranked by significance (holdings, standards). Each <=400 chars.’ Ensure cost monitoring via logging.”
These prompts are designed for sequential implementation, starting with the highest priorities. After applying each, test the chatbot with sample legal queries to verify maintained accuracy and anti-hallucination features. If further adjustments are required based on testing outcomes, provide details for refinement.